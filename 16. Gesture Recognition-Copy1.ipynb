{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import datetime, os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda update anaconda-navigator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    #loading dataframes\n",
    "    train_df = pd.read_csv('./input/boneage-training-dataset.csv')\n",
    "    test_df = pd.read_csv('./input/boneage-test-dataset.csv')\n",
    "\n",
    "    #appending file extension to id column for both training and testing dataframes\n",
    "    train_df['id'] = train_df['id'].apply(lambda x: str(x)+'.png')\n",
    "    test_df['Case ID'] = test_df['Case ID'].apply(lambda x: str(x)+'.png') \n",
    "    train_df['gender'] = train_df['male'].apply(lambda x: 'male' if x else 'female')\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our Training and Test Data\n",
    "train_df,_ = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/boneage-training-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belongs_to = np.ones(len(df['boneage']))\n",
    "for k, age in tqdm(enumerate(df['boneage'])):\n",
    "    if age <= 3:\n",
    "        belongs_to[k] = 1\n",
    "    elif age > 3  and  age <= 6:\n",
    "        belongs_to[k] = 2\n",
    "    elif age > 6  and  age <= 9:\n",
    "        belongs_to[k] = 3\n",
    "    elif age > 9  and  age <= 12:\n",
    "        belongs_to[k] = 4\n",
    "    elif age > 12  and  age <= 15:\n",
    "        belongs_to[k] = 5\n",
    "    elif age > 15  and  age <= 18:\n",
    "        belongs_to[k] = 6\n",
    "    elif age > 18  and  age <= 21:\n",
    "        belongs_to[k] = 7\n",
    "    elif age > 21  and  age <= 24:\n",
    "        belongs_to[k] = 8\n",
    "    elif age > 24  and  age <= 30:\n",
    "        belongs_to[k] = 9\n",
    "    elif age > 30  and  age <= 36:\n",
    "        belongs_to[k] = 10\n",
    "    elif age > 36  and  age <= 42:\n",
    "        belongs_to[k] = 11\n",
    "    elif age > 42  and  age <= 48:\n",
    "        belongs_to[k] = 12\n",
    "    elif age > 48  and  age <= 54:\n",
    "        belongs_to[k] = 13\n",
    "    elif age > 54  and  age <= 60:\n",
    "        belongs_to[k] = 14\n",
    "    elif age > 60  and  age <= 66:\n",
    "        belongs_to[k] = 15\n",
    "    elif age > 66  and  age <= 72:\n",
    "        belongs_to[k] = 16\n",
    "    elif age > 72  and  age <= 78:\n",
    "        belongs_to[k] = 17\n",
    "    elif age > 78  and  age <= 84:\n",
    "        belongs_to[k] = 18\n",
    "    elif age > 84  and  age <= 90:\n",
    "        belongs_to[k] = 19\n",
    "    elif age > 90  and  age <= 96:\n",
    "        belongs_to[k] = 20\n",
    "    elif age > 96  and  age <= 102:\n",
    "        belongs_to[k] = 21\n",
    "    elif age > 102  and  age <= 108:\n",
    "        belongs_to[k] = 22\n",
    "    elif age > 108  and  age <= 114:\n",
    "        belongs_to[k] = 23\n",
    "    elif age > 114  and  age <= 120:\n",
    "        belongs_to[k] = 24\n",
    "    elif age > 120  and  age <= 132:\n",
    "        belongs_to[k] = 25\n",
    "    elif age > 132  and  age <= 144:\n",
    "        belongs_to[k] = 26\n",
    "    elif age > 144  and  age <= 156:\n",
    "        belongs_to[k] = 27\n",
    "    elif age > 156  and  age <= 168:\n",
    "        belongs_to[k] = 28\n",
    "    elif age > 168  and  age <= 180:\n",
    "        belongs_to[k] = 29\n",
    "    elif age > 180  and  age <= 192:\n",
    "        belongs_to[k] = 30\n",
    "    elif age > 192  and  age <= 204:\n",
    "        belongs_to[k] = 31\n",
    "    elif age > 204  and  age <= 216:\n",
    "        belongs_to[k] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['belongs_to_group'] = belongs_to.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['belongs_to_group'] = pd.to_numeric(df['belongs_to_group'], downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './new_dataset'\n",
    "print(len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "length = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['belongs_to_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = (128 * 128)\n",
    "data = np.ones(length * data_size)\n",
    "data = data.reshape((length, 128 , 128))\n",
    "new_y = np.ones(length)\n",
    "count = 0\n",
    "cnt = 0\n",
    "\n",
    "for filename, boneage in tqdm(list(train_df[['id','boneage']].values)):\n",
    "    try:\n",
    "        fname = './new_dataset/'+ filename\n",
    "        img = cv2.imread(fname)\n",
    "        img  = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, (128, 128), interpolation = cv2.INTER_AREA)\n",
    "        new_y[count] = y[cnt]\n",
    "        data[count,:,:] = img.tolist()\n",
    "        count += 1\n",
    "    except Exception as ex:\n",
    "        continue\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=data.reshape(length,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "cnt=-1\n",
    "for filename, boneage in tqdm(list(train_df[['id','boneage']].values)):\n",
    "    cnt += 1\n",
    "    fname = './new_dataset/'+ filename\n",
    "    if os.path.isfile(fname):\n",
    "        new_y[count] = y[cnt]\n",
    "        count += 1        \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our training labels\n",
    "labels = new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the unique labels, 32 in total\n",
    "unique_val = np.array(labels)\n",
    "np.unique(unique_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot one encode our labels\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binrizer = LabelBinarizer()\n",
    "labels = label_binrizer.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View our labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an image\n",
    "index = 13\n",
    "print(labels[index])\n",
    "plt.imshow(images[index].reshape(128,128))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into x_train, x_test, y_train and y_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start loading our tensorFlow modules and define our batch size etc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our images\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape them into the size required by TF and Keras\n",
    "x_train = x_train.reshape(x_train.shape[0], 128, 128, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 128, 128, 1)\n",
    "\n",
    "plt.imshow(x_train[0].reshape(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our CNN Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu', input_shape=(128, 128 ,1) ))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(1024, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile our Model\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer= Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our Model\n",
    "history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our Model\n",
    "model.save(\"sign_mnist_cnn_50_Epochs.h5\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View our training history graphically\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train','test'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape our test data so that we can evaluate it's performance on unseen data\n",
    "test_labels = test['label']\n",
    "test.drop('label', axis = 1, inplace = True)\n",
    "\n",
    "test_images = test.values\n",
    "test_images = np.array([np.reshape(i, (28, 28)) for i in test_images])\n",
    "test_images = np.array([i.flatten() for i in test_images])\n",
    "\n",
    "test_labels = label_binrizer.fit_transform(test_labels)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "test_images.shape\n",
    "\n",
    "y_pred = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test_labels, y_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to match label to letter\n",
    "def getLetter(result):\n",
    "    classLabels = { 0: 'A',\n",
    "                    1: 'B',\n",
    "                    2: 'C',\n",
    "                    3: 'D',\n",
    "                    4: 'E',\n",
    "                    5: 'F',\n",
    "                    6: 'G',\n",
    "                    7: 'H',\n",
    "                    8: 'I',\n",
    "                    9: 'K',\n",
    "                    10: 'L',\n",
    "                    11: 'M',\n",
    "                    12: 'N',\n",
    "                    13: 'O',\n",
    "                    14: 'P',\n",
    "                    15: 'Q',\n",
    "                    16: 'R',\n",
    "                    17: 'S',\n",
    "                    18: 'T',\n",
    "                    19: 'U',\n",
    "                    20: 'V',\n",
    "                    21: 'W',\n",
    "                    22: 'X',\n",
    "                    23: 'Y'}\n",
    "    try:\n",
    "        res = int(result)\n",
    "        return classLabels[res]\n",
    "    except:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Actual Webcam Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    ##############################\n",
    "    frame=cv2.flip(frame, 1)\n",
    "\n",
    "    #define region of interest\n",
    "    roi = frame[100:400, 320:620]\n",
    "    cv2.imshow('roi', roi)\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    roi = cv2.resize(roi, (28, 28), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    cv2.imshow('roi sacled and gray', roi)\n",
    "    copy = frame.copy()\n",
    "    cv2.rectangle(copy, (320, 100), (620, 400), (255,0,0), 5)\n",
    "    \n",
    "    roi = roi.reshape(1,28,28,1) \n",
    "\n",
    "    result = str(model.predict_classes(roi, 1, verbose = 0)[0])\n",
    "    cv2.putText(copy, getLetter(result), (300 , 100), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 255, 0), 2)\n",
    "    cv2.imshow('frame', copy)    \n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance is less than adequate because our training data is very different from our real life data\n",
    "\n",
    "Let's create our own dataset using your webcam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to setup the directories we'll be storing our images\n",
    "def makedir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return None\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use OpenCV to Build our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "i=0\n",
    "image_count = 0\n",
    "    \n",
    "while i < 7:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    #define region of interest\n",
    "    roi = frame[100:400, 320:620]\n",
    "    cv2.imshow('roi', roi)\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    roi = cv2.resize(roi, (28, 28), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    cv2.imshow('roi sacled and gray', roi)\n",
    "    copy = frame.copy()\n",
    "    cv2.rectangle(copy, (320, 100), (620, 400), (255,0,0), 5)\n",
    "    \n",
    "    if i == 0:\n",
    "        image_count = 0\n",
    "        cv2.putText(copy, \"Hit Enter to Record when ready\", (100 , 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "    if i == 1:\n",
    "        image_count+=1\n",
    "        cv2.putText(copy, \"Recording 1st gesture - Train\", (100 , 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        cv2.putText(copy, str(image_count), (400 , 400), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        gesture_one = './handgestures/train/0/'\n",
    "        makedir(gesture_one)\n",
    "        cv2.imwrite(gesture_one + str(image_count) + \".jpg\", roi)\n",
    "    if i == 2:\n",
    "        image_count+=1\n",
    "        cv2.putText(copy, \"Recording 1st gesture - Test\", (100 , 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        cv2.putText(copy, str(image_count), (400 , 400), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        gesture_one = './handgestures/test/0/'\n",
    "        makedir(gesture_one)\n",
    "        cv2.imwrite(gesture_one + str(image_count) + \".jpg\", roi)\n",
    "    if i == 3:\n",
    "        cv2.putText(copy, \"Hit Enter to Record when ready to Record 2nd gesture\", (100 , 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "    if i == 4:\n",
    "        image_count+=1\n",
    "        cv2.putText(copy, \"Recording 2nd gesture - Train\", (100 , 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        cv2.putText(copy, str(image_count), (400 , 400), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        gesture_two = './handgestures/train/1/'\n",
    "        makedir(gesture_two)\n",
    "        cv2.imwrite(gesture_two + str(image_count) + \".jpg\", roi)\n",
    "    if i == 5:\n",
    "        image_count+=1\n",
    "        cv2.putText(copy, \"Recording 2nd gesture - Test\", (100 , 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        cv2.putText(copy, str(image_count), (400 , 400), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "        gesture_two = './handgestures/test/1/'\n",
    "        makedir(gesture_two)\n",
    "        cv2.imwrite(gesture_two + str(image_count) + \".jpg\", roi)\n",
    "    if i == 6:\n",
    "        cv2.putText(copy, \"Hit Enter to Exit\", (100 , 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 1)\n",
    "    cv2.imshow('frame', copy)    \n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        image_count = 0\n",
    "        i+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Keras's Data Augmentation to enhance our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "num_classes = 2\n",
    "img_rows, img_cols = 28, 28\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = './handgestures/train'\n",
    "validation_data_dir = './handgestures/test'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      width_shift_range=0.3,\n",
    "      height_shift_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='binary')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu', input_shape=(28,28,1) ))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a very small learning rate \n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'rmsprop',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 1206 \n",
    "nb_validation_samples = 301 \n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our Model\n",
    "model.save(\"my_gestures_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "classifier = load_model('my_gestures_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's test the model we just built!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    ##############################\n",
    "    frame=cv2.flip(frame, 1)\n",
    "\n",
    "    #define region of interest\n",
    "    roi = frame[100:400, 320:620]\n",
    "    cv2.imshow('roi', roi)\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    roi = cv2.resize(roi, (28, 28), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    cv2.imshow('roi scaled and gray', roi)\n",
    "    copy = frame.copy()\n",
    "    cv2.rectangle(copy, (320, 100), (620, 400), (255,0,0), 5)\n",
    "    \n",
    "    roi = roi.reshape(1,28,28,1) \n",
    "    roi = roi/255\n",
    "    result = str(classifier.predict_classes(roi, 1, verbose = 0)[0])\n",
    "    cv2.putText(copy, str(result), (300 , 100), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 255, 0), 2)\n",
    "    cv2.imshow('frame', copy)    \n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
